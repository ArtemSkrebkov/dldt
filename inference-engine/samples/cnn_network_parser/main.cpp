// Copyright (C) 2018-2019 Intel Corporation
// SPDX-License-Identifier: Apache-2.0
//

#include <vector>
#include <memory>
#include <string>

#include <inference_engine.hpp>

using namespace InferenceEngine;

int main(int argc, char *argv[]) {
    try {
        // ------------------------------ Parsing and validation of input args ---------------------------------
        if (argc != 2) {
		std::cout << "Usage : ./cnn_network_parser <path_to_model>" << std::endl;
            return EXIT_FAILURE;
        }

        const file_name_t input_model{argv[1]};

        // --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
        CNNNetReader network_reader;
        network_reader.ReadNetwork(fileNameToString(input_model));
        network_reader.ReadWeights(fileNameToString(input_model).substr(0, input_model.size() - 4) + ".bin");
        network_reader.getNetwork().setBatchSize(1);
        CNNNetwork network = network_reader.getNetwork();
	for (auto &&layer : network) {
	    std::cout << "Layer name: "<< layer->name << '\n';
	    std::cout << "Parameters:" << '\n';
	    for (auto &&param : layer->params) {
                std::cout << '\t' << param.first << ": " << param.second << '\n';
	    }
	    for (auto &&blob : layer->blobs) {
                std::cout << '\t' << "First three bytes of " << blob.first << ": " 
			                               << static_cast<int>(blob.second->buffer().as<char *>()[0]) << ' '
                                                       << static_cast<int>(blob.second->buffer().as<char *>()[1]) << ' '
                                                       << static_cast<int>(blob.second->buffer().as<char *>()[2]) << '\n';
	    } 
	    std::cout << '\n';
	}
        // -----------------------------------------------------------------------------------------------------
    } catch (const std::exception & ex) {
        std::cerr << ex.what() << std::endl;
        return EXIT_FAILURE;
    }
    return EXIT_SUCCESS;
}
